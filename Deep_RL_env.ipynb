{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jacob\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf  \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import tensorflow_addons.layers as layers\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from gym import spaces\n",
    "\n",
    "import math\n",
    "\n",
    "try:\n",
    "    xrange = xrange\n",
    "except:\n",
    "    xrange = range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./seizure_data_no_patient.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X170</th>\n",
       "      <th>X171</th>\n",
       "      <th>X172</th>\n",
       "      <th>X173</th>\n",
       "      <th>X174</th>\n",
       "      <th>X175</th>\n",
       "      <th>X176</th>\n",
       "      <th>X177</th>\n",
       "      <th>X178</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>190</td>\n",
       "      <td>229</td>\n",
       "      <td>223</td>\n",
       "      <td>192</td>\n",
       "      <td>125</td>\n",
       "      <td>55</td>\n",
       "      <td>-9</td>\n",
       "      <td>-33</td>\n",
       "      <td>-38</td>\n",
       "      <td>...</td>\n",
       "      <td>-17</td>\n",
       "      <td>-15</td>\n",
       "      <td>-31</td>\n",
       "      <td>-77</td>\n",
       "      <td>-103</td>\n",
       "      <td>-127</td>\n",
       "      <td>-116</td>\n",
       "      <td>-83</td>\n",
       "      <td>-51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>386</td>\n",
       "      <td>382</td>\n",
       "      <td>356</td>\n",
       "      <td>331</td>\n",
       "      <td>320</td>\n",
       "      <td>315</td>\n",
       "      <td>307</td>\n",
       "      <td>272</td>\n",
       "      <td>244</td>\n",
       "      <td>232</td>\n",
       "      <td>...</td>\n",
       "      <td>164</td>\n",
       "      <td>150</td>\n",
       "      <td>146</td>\n",
       "      <td>152</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>154</td>\n",
       "      <td>143</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-32</td>\n",
       "      <td>-39</td>\n",
       "      <td>-47</td>\n",
       "      <td>-37</td>\n",
       "      <td>-32</td>\n",
       "      <td>-36</td>\n",
       "      <td>-57</td>\n",
       "      <td>-73</td>\n",
       "      <td>-85</td>\n",
       "      <td>-94</td>\n",
       "      <td>...</td>\n",
       "      <td>57</td>\n",
       "      <td>64</td>\n",
       "      <td>48</td>\n",
       "      <td>19</td>\n",
       "      <td>-12</td>\n",
       "      <td>-30</td>\n",
       "      <td>-35</td>\n",
       "      <td>-35</td>\n",
       "      <td>-36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-105</td>\n",
       "      <td>-101</td>\n",
       "      <td>-96</td>\n",
       "      <td>-92</td>\n",
       "      <td>-89</td>\n",
       "      <td>-95</td>\n",
       "      <td>-102</td>\n",
       "      <td>-100</td>\n",
       "      <td>-87</td>\n",
       "      <td>-79</td>\n",
       "      <td>...</td>\n",
       "      <td>-82</td>\n",
       "      <td>-81</td>\n",
       "      <td>-80</td>\n",
       "      <td>-77</td>\n",
       "      <td>-85</td>\n",
       "      <td>-77</td>\n",
       "      <td>-72</td>\n",
       "      <td>-69</td>\n",
       "      <td>-65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-9</td>\n",
       "      <td>-65</td>\n",
       "      <td>-98</td>\n",
       "      <td>-102</td>\n",
       "      <td>-78</td>\n",
       "      <td>-48</td>\n",
       "      <td>-16</td>\n",
       "      <td>0</td>\n",
       "      <td>-21</td>\n",
       "      <td>-59</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-12</td>\n",
       "      <td>-32</td>\n",
       "      <td>-41</td>\n",
       "      <td>-65</td>\n",
       "      <td>-83</td>\n",
       "      <td>-89</td>\n",
       "      <td>-73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 179 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    X1   X2   X3   X4   X5   X6   X7   X8   X9  X10  ...  X170  X171  X172  \\\n",
       "0  135  190  229  223  192  125   55   -9  -33  -38  ...   -17   -15   -31   \n",
       "1  386  382  356  331  320  315  307  272  244  232  ...   164   150   146   \n",
       "2  -32  -39  -47  -37  -32  -36  -57  -73  -85  -94  ...    57    64    48   \n",
       "3 -105 -101  -96  -92  -89  -95 -102 -100  -87  -79  ...   -82   -81   -80   \n",
       "4   -9  -65  -98 -102  -78  -48  -16    0  -21  -59  ...     4     2   -12   \n",
       "\n",
       "   X173  X174  X175  X176  X177  X178  y  \n",
       "0   -77  -103  -127  -116   -83   -51  0  \n",
       "1   152   157   156   154   143   129  1  \n",
       "2    19   -12   -30   -35   -35   -36  0  \n",
       "3   -77   -85   -77   -72   -69   -65  0  \n",
       "4   -32   -41   -65   -83   -89   -73  0  \n",
       "\n",
       "[5 rows x 179 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reward_adjustment = 3\n",
    "# Refers to Reward structure. Is imbalance Ratio of classes.\n",
    "# This ratio/reward maybe tuned to improve model Recall\n",
    "lamb_da = (1908/10422) * Reward_adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5492227979274611"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lamb_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameEnv(gym.Env):\n",
    " \n",
    "    def __init__(self):\n",
    "        \n",
    "        #Init Action Space with to Discrete steps. 0 or 1, Left or right, On or off.\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        #Init Observation space 26x1 that contain continous values. change shape to fit any dataframe(in theory)\n",
    "        self.observation_space = spaces.Box(-np.inf, np.inf, shape=(178,), dtype=np.float32)\n",
    "        \n",
    "        #Init the rest of DataFrameEnv class variables\n",
    "        self.episode_df = None\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "        self.episode = None\n",
    "        self.test_set_no_labels = None\n",
    "        self.test_labels = None\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset function randomly samples Dataframe, Splits off labels and converts to list\n",
    "        self.episode_df = df.sample(frac=0.8)\n",
    "        self.episode = self.episode_df.drop(columns=['y']).values.tolist()\n",
    "        self.true_labels = self.episode_df['y'].values.tolist()\n",
    "        \n",
    "        try:\n",
    "            # Remove one row from sampled DF \n",
    "            self.state = self.episode.pop()\n",
    "        except:\n",
    "            self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "        # Return to agent as a state.\n",
    "        return np.array(self.state)\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))\n",
    "        \n",
    "        # Pull off next row and assign to state to be returned to agent from this function\n",
    "        state = self.episode.pop()\n",
    "        \n",
    "\n",
    "        done = False\n",
    "        true_label = self.true_labels.pop()\n",
    "        # Compare agents prediction with label and assign reward\n",
    "        \n",
    "        if true_label is not None:\n",
    "            \n",
    "            if (true_label == 1) & (action == true_label):\n",
    "                reward = 1\n",
    "            \n",
    "            elif (true_label == 0) & (action == true_label):\n",
    "                reward = lamb_da\n",
    "            \n",
    "            elif (true_label == 0) & (action != true_label):\n",
    "                reward = -lamb_da\n",
    "            \n",
    "            else:\n",
    "                reward = -1\n",
    "                done = True\n",
    "        else:\n",
    "            reward = 0\n",
    "            done = True\n",
    "        # Return Next state, reward for action in current state and if done/ stop episode.\n",
    "        return np.array(state), reward, done, {}\n",
    "\n",
    "    def validate(self):\n",
    "        # Validation function compare predictions to actuals on test set to build Confusion Matrix\n",
    "        test_set = df[~df.index.isin(self.episode_df.index)]\n",
    "        self.test_set_no_labels = test_set.drop(columns=['y'])\n",
    "        self.test_labels = test_set['y']\n",
    "        \n",
    "        return self.test_set_no_labels, self.test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Environment\n",
    "env = DataFrameEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "H = 178 # number of hidden layer neurons\n",
    "batch_size = 6 # how many episodes before doing a policy update\n",
    "learning_rate = 0.009 # How big/small the steps are in the gradient ascent\n",
    "gamma = 0.99 # discount factor for reward\n",
    "learning_decay = 50 #Learning rate decay (not used currently)\n",
    "D = 178 # input dimensionality\n",
    "total_episodes = 1000 #duhh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    # Calculates a reward to reduce rewards gained towards the end of an episode.\n",
    "    # Not used except for gamma which is used to regulate prioritization of current vs future rewards.\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "#This defines the network as it goes from taking an observation of the environment to \n",
    "#giving a probability of chosing to the action 0 or 1.\n",
    "observations = tf.compat.v1.placeholder(tf.float32, [None,D] , name=\"input_x\")\n",
    "W1 = tf.compat.v1.get_variable(\"W1\", shape=[D, H],\n",
    "           initializer=tf.initializers.GlorotUniform())\n",
    "\n",
    "layer1 = tf.nn.tanh(tf.matmul(observations,W1))\n",
    "W11 = tf.compat.v1.get_variable(\"W11\", shape=[H, 89],\n",
    "           initializer=tf.initializers.GlorotUniform())\n",
    "\n",
    "\n",
    "layer11 = tf.nn.relu(tf.matmul(layer1, W11))\n",
    "W2 = tf.compat.v1.get_variable(\"W2\", shape=[89, 1],\n",
    "           initializer=tf.initializers.GlorotUniform())\n",
    "\n",
    "score = tf.matmul(layer11,W2)\n",
    "\n",
    "probability = tf.nn.sigmoid(score)\n",
    "\n",
    "#From here we define the parts of the network needed for learning a good policy.\n",
    "tvars = tf.compat.v1.trainable_variables()\n",
    "input_y = tf.compat.v1.placeholder(tf.float32,[None,1], name=\"input_y\")\n",
    "advantages = tf.compat.v1.placeholder(tf.float32,name=\"reward_signal\")\n",
    "\n",
    "# The loss function. This sends the weights in the direction of making actions \n",
    "# that gave good advantage (reward over time) more likely, and actions that didn't less likely.\n",
    "loglik = tf.compat.v1.log(input_y*(input_y - probability) + (1 - input_y)*(input_y + probability))\n",
    "loss = -tf.reduce_mean(loglik * advantages)\n",
    "\n",
    "newGrads = tf.gradients(loss,tvars)\n",
    "\n",
    "# Once we have collected a series of gradients from multiple episodes, we apply them.\n",
    "# We don't just apply gradeients after every episode in order to account for noise in the reward signal.\n",
    "adam = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate) # Our optimizer\n",
    "W1Grad = tf.compat.v1.placeholder(tf.float32,name=\"batch_grad1\")\n",
    "W11Grad = tf.compat.v1.placeholder(tf.float32,name=\"batch_grad11\")# Placeholders to send the final gradients through when we update.\n",
    "W2Grad = tf.compat.v1.placeholder(tf.float32,name=\"batch_grad2\")\n",
    "batchGrad = [W1Grad,W11Grad,W2Grad]\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad,tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stores states, actions, rewards for an episode\n",
    "xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "episode_rewards = []\n",
    "\n",
    "#lists that maybe used to visualise model training/updates\n",
    "policy_gradient = []\n",
    "Q_approx = []\n",
    "loss_list = []\n",
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for episode -1.000000.  Total average reward -1.000000.\n",
      "Average reward for episode -1.000000.  Total average reward -1.000000.\n",
      "Average reward for episode -1.000000.  Total average reward -1.000000.\n",
      "Average reward for episode 0.000000.  Total average reward -1.000000.\n",
      "Average reward for episode 0.000000.  Total average reward -1.000000.\n",
      "Average reward for episode 0.000000.  Total average reward -1.000000.\n",
      "Average reward for episode 2.000000.  Total average reward -1.000000.\n",
      "Average reward for episode 1.000000.  Total average reward -1.000000.\n",
      "Average reward for episode 3.000000.  Total average reward -1.000000.\n",
      "Average reward for episode 0.000000.  Total average reward -1.000000.\n",
      "Average reward for episode -1.000000.  Total average reward -1.000000.\n",
      "Average reward for episode 1.000000.  Total average reward -1.000000.\n",
      "Average reward for episode -1.000000.  Total average reward -1.000000.\n",
      "Average reward for episode 1.000000.  Total average reward -1.000000.\n",
      "Average reward for episode 2.000000.  Total average reward -1.000000.\n",
      "Average reward for episode 1.000000.  Total average reward -1.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 3.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 3.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 3.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 3.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 4.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 3.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 3.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 3.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 3.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 4.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 3.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 5.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 3.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 5.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 3.000000.  Total average reward 1.000000.\n",
      "Average reward for episode 2.000000.  Total average reward 1.000000.\n",
      "Average reward for episode 3.000000.  Total average reward 1.000000.\n",
      "Average reward for episode 2.000000.  Total average reward 1.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 1.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 1.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 1.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 1.000000.\n",
      "Average reward for episode 2.000000.  Total average reward 1.000000.\n",
      "Average reward for episode 2.000000.  Total average reward 1.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 1.000000.\n",
      "Average reward for episode -4.000000.  Total average reward 1.000000.\n",
      "Average reward for episode 2.000000.  Total average reward 1.000000.\n",
      "Average reward for episode -2.000000.  Total average reward 1.000000.\n",
      "Average reward for episode -3.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -3.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -3.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -3.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -4.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -3.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -5.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -3.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -3.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -3.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -3.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -3.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -3.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -3.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -3.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -7.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -3.000000.  Total average reward 0.000000.\n",
      "Average reward for episode -2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "1001 Episodes completed.\n"
     ]
    }
   ],
   "source": [
    "init = tf.compat.v1.global_variables_initializer()\n",
    "# Launch the graph\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    observation = env.reset() # Obtain an initial observation of the environment\n",
    "\n",
    "    # Reset the gradient placeholder. We will collect gradients in \n",
    "    # gradBuffer until we are ready to update our policy network. \n",
    "    gradBuffer = sess.run(tvars)\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    \n",
    "    while episode_number <= total_episodes:\n",
    "        \n",
    "        # Make sure the observation is in a shape the network can handle.\n",
    "        x = np.reshape(observation,[1,D])\n",
    "        \n",
    "        # Run the policy network and get an action to take. \n",
    "        tfprob = sess.run(probability,feed_dict={observations: x})\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "        \n",
    "        \n",
    "        xs.append(x) # observation\n",
    "        y = 1 if action == 0 else 0 # a \"fake label\"\n",
    "        ys.append(y)\n",
    "\n",
    "        # step the environment and get new measurements\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        reward_sum += reward\n",
    "\n",
    "        drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "        \n",
    "        \n",
    "        if done: \n",
    "            episode_number += 1\n",
    "            # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            episode_rewards.append(np.sum(drs))\n",
    "            tfp = tfps\n",
    "            xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[] # reset array memory\n",
    "\n",
    "            # compute the discounted reward backwards through time\n",
    "            discounted_epr = discount_rewards(epr)\n",
    "            \n",
    "            \n",
    "            # Get the gradient for this episode, and save it in the gradBuffer\n",
    "            tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n",
    "            for ix,grad in enumerate(tGrad):\n",
    "                gradBuffer[ix] += grad\n",
    "                \n",
    "            # If we have completed enough episodes, then update the policy network with our gradients.\n",
    "            if episode_number % batch_size == 0: \n",
    "                policy_gradient.append(gradBuffer)\n",
    "                sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W11Grad:gradBuffer[1],W2Grad:gradBuffer[2]})\n",
    "\n",
    "                loss_list.append(loss)\n",
    "                for ix,grad in enumerate(gradBuffer):\n",
    "                    gradBuffer[ix] = grad * 0\n",
    "                \n",
    "                # Give a summary of how well our network is doing for each batch of episodes.\n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "                \n",
    "                print('Average reward for episode %f.  Total average reward %f.' % (reward_sum//batch_size, running_reward//batch_size))\n",
    "                \n",
    "                reward_sum = 0\n",
    "\n",
    "            observation = env.reset()\n",
    "    #get test set with labels. \n",
    "    test_set, test_labels = env.validate()\n",
    "    \n",
    "    for i in test_set.values:\n",
    "        # predict on each test row and append to list\n",
    "        prediction = sess.run(probability, feed_dict={observations:np.reshape(i,[1,D])})\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "            \n",
    "print(episode_number,'Episodes completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_clean = []\n",
    "for i in predictions:\n",
    "    if i < .50:\n",
    "        i = 0\n",
    "        pred_clean.append(i)\n",
    "    else:\n",
    "        i = 1\n",
    "        pred_clean.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_report = classification_report(test_labels,pred_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.32      0.47      1843\n",
      "           1       0.22      0.78      0.35       457\n",
      "\n",
      "    accuracy                           0.41      2300\n",
      "   macro avg       0.54      0.55      0.41      2300\n",
      "weighted avg       0.73      0.41      0.44      2300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = confusion_matrix(test_labels,pred_clean,normalize='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25913043, 0.54217391],\n",
       "       [0.04391304, 0.15478261]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
